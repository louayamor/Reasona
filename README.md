# Reasona

**Reasona** is a modular AI/ML pipeline framework designed for processing, cleaning, transforming, and training on synthetic datasets. It provides end-to-end capabilities for data ingestion, preprocessing, model training, and inference with structured configuration.

---

## ğŸš€ Features

- **Data Ingestion**: Stream and combine large datasets efficiently, optimized for limited RAM (16GB tested).  
- **Data Cleaning**: Remove duplicates, handle missing values, and preprocess data for downstream tasks.  
- **Data Transformation**: Format datasets into instruction-based formats suitable for model training.  
- **Training & Fine-Tuning**: Support for LoRA-based fine-tuning pipelines.  
- **Inference**: Flexible inference engine integration with customizable parameters.  
- **Configuration Management**: Centralized YAML-based configuration for preprocessing, training, and inference.  

---

## ğŸ’¾ Project Structure

```
Reasona/
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ Reasona/
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â”œâ”€â”€ config_manager.py
â”‚       â”‚   â””â”€â”€ params.yaml
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â”œâ”€â”€ loader.py
â”‚       â”‚   â”œâ”€â”€ cleaner.py
â”‚       â”‚   â”œâ”€â”€ formatter.py
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ pipeline/
â”‚       â”‚   â”œâ”€â”€ preprocess_pipeline.py
â”‚       â”‚   â”œâ”€â”€ training_pipeline.py
â”‚       â”‚   â””â”€â”€ inference_pipeline.py
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ logger.py
â”‚           â””â”€â”€ helpers.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ params.yaml
â”œâ”€â”€ artifacts/                # Generated datasets and outputs
â”œâ”€â”€ main.py                   # Entry point to run pipelines
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

1. Clone the repository:

```bash
git clone https://github.com/yourusername/Reasona.git
cd Reasona
```

2. Create and activate a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ’ª Usage

### Run Preprocessing Pipeline

```bash
python main.py
```

This will execute the preprocessing pipeline including:

1. **Data Ingestion** â€“ combining dataset files (streaming from Hugging Face if configured).  
2. **Data Cleaning** â€“ removing duplicates and handling missing values.  
3. **Data Transformation** â€“ formatting into instruction-based JSON for training.  

### Training and Inference

After preprocessing, configure `config.yaml` and `params.yaml` for model training or inference pipelines. Then run:

```bash
python src/Reasona/pipeline/training_pipeline.py
python src/Reasona/pipeline/inference_pipeline.py
```

---

## ğŸŒ Data Source

- The pipeline supports streaming large datasets directly from [Hugging Face Datasets](https://huggingface.co/datasets).  
- Example dataset used: [`PleIAs/SYNTH`](https://huggingface.co/datasets/PleIAs/SYNTH)  

---

## ğŸ‘¤ Author

**Louay Amor** â€“ [GitHub](https://github.com/louayamor) | [LinkedIn](https://linkedin.com/in/louayamor)

